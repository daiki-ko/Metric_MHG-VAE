{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import vectorize as vec\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole \n",
    "from rdkit.Chem import Descriptors,PandasTools\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm9_df = pd.read_csv('datasets/qm9/qm9_shuffle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0      int64\n",
      "mol_id         object\n",
      "smiles         object\n",
      "A             float64\n",
      "B             float64\n",
      "C             float64\n",
      "mu            float64\n",
      "alpha         float64\n",
      "homo          float64\n",
      "lumo          float64\n",
      "gap           float64\n",
      "r2            float64\n",
      "zpve          float64\n",
      "u0            float64\n",
      "u298          float64\n",
      "h298          float64\n",
      "g298          float64\n",
      "cv            float64\n",
      "u0_atom       float64\n",
      "u298_atom     float64\n",
      "h298_atom     float64\n",
      "g298_atom     float64\n",
      "dtype: object\n",
      "Index(['Unnamed: 0', 'mol_id', 'smiles', 'A', 'B', 'C', 'mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'u0', 'u298', 'h298', 'g298', 'cv', 'u0_atom', 'u298_atom', 'h298_atom', 'g298_atom'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "column_type = qm9_df.dtypes\n",
    "column_name = qm9_df.columns\n",
    "print(column_type)\n",
    "print(column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Embedding Target Property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vec = pd.read_csv('datasets/qm9/qm9_shuffle.csv',usecols=['homo'])\n",
    "target_vec = target_vec.astype('float')\n",
    "target_vec = target_vec.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "\n",
    "for tgt in target_vec:\n",
    "    target_list.append(tgt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "\n",
    "    # Target scaling for HOMO energy\n",
    "    # The scale of HOMO energyã€€property values is very small\n",
    "for tgt in target_vec:\n",
    "    target_list.append(tgt[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-23.59, -23.02, -24.169999999999998, -24.52, -25.27, -26.85, -25.46, -24.22, -23.02, -23.400000000000002]\n"
     ]
    }
   ],
   "source": [
    "print(target_list[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load input data and model\n",
      "load completed\n"
     ]
    }
   ],
   "source": [
    "from mhg.nn.autoencoder import GrammarSeq2SeqVAE\n",
    "from mhg.nn.dataset import HRGDataset, batch_padding\n",
    "from mhg.smi import HGGen, hg_to_mol\n",
    "from mhg.hrg import HyperedgeReplacementGrammar as HRG\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "def get_dataloaders(hrg, prod_rule_seq_list, target_val_list=None, batch_size=None, shuffle=False):\n",
    "    \n",
    "    ''' return a dataloader for train/val/test\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prod_rule_seq_list : List of lists\n",
    "        each element corresponds to a sequence of production rules.\n",
    "    train_params : dict\n",
    "        self.Train_params\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataloaders for train, val, test of autoencoders\n",
    "        each batch contains two torch Tensors, each of which corresponds to input and output of autoencoder.\n",
    "    '''\n",
    "    # Parameters for training a variational autoencoder\n",
    "    Train_params = {\n",
    "        'model': 'GrammarSeq2SeqVAE', # Model name\n",
    "        'model_params' : { # Parameter for the model\n",
    "            'latent_dim': 50, # Dimension of the latent dimension\n",
    "            'max_len': 12, # maximum length of input sequences (represented as sequences of production rules)\n",
    "            'batch_size': 64, # batch size for training\n",
    "            'padding_idx': -1, # integer used for padding\n",
    "            'start_rule_embedding': False, # Embed the starting rule into the latent dimension explicitly\n",
    "            'encoder_name': 'GRU', # Type of encoder\n",
    "            'encoder_params': {'hidden_dim': 384, # hidden dim\n",
    "                               'num_layers': 3, # the number of layers\n",
    "                               'bidirectional': True, # use bidirectional one or not\n",
    "                               'dropout': 0.1}, # dropout probability\n",
    "            'decoder_name': 'GRU', # Type of decoder\n",
    "            'decoder_params': {'hidden_dim': 384, # hidden dim\n",
    "                               'num_layers': 3, # the number of layers\n",
    "                               'dropout': 0.1}, # dropout probability\n",
    "            'prod_rule_embed': ['Embedding',\n",
    "                                'MolecularProdRuleEmbedding',\n",
    "                                'MolecularProdRuleEmbeddingLastLayer',\n",
    "                                'MolecularProdRuleEmbeddingUsingFeatures'][0], # Embedding method of a production rule. The simple embedding was the best, but each production rule could be embedded using GNN\n",
    "            'prod_rule_embed_params': {'out_dim': 900, # embedding dimension\n",
    "                                       'layer2layer_activation': 'relu', # not used for `Embedding`\n",
    "                                       'layer2out_activation': 'softmax', # not used for `Embedding`\n",
    "                                       'num_layers': 4}, # not used for `Embedding`\n",
    "            'criterion_func': ['VAELoss', 'GrammarVAELoss'][1], # Loss function\n",
    "            'criterion_func_kwargs': {'beta': 0.01}}, # Parameters for the loss function. `beta` specifies beta-VAE.\n",
    "        'sgd': 'Adam', # SGD algorithm\n",
    "        'sgd_params': {'lr': 5e-4 # learning rate of SGD\n",
    "        },\n",
    "        #'seed_list': [141, 123, 425, 552, 1004, 50243], # seeds used for restarting training\n",
    "        'seed_list': [128], # seeds used for restarting training\n",
    "        'inversed_input': True, # the input sequence is in the reversed order or not.\n",
    "        'num_train':99968, # the number of training examples\n",
    "        'num_val': 1000, # the number of validation examples\n",
    "        'num_test': 28928, # the number of test examples\n",
    "        'num_early_stop': 220011, # the number of examples used to find better initializations (=seed)\n",
    "        'num_epochs': 10 # the number of training epochs\n",
    "    }\n",
    "    \n",
    "    train_params = Train_params\n",
    "    \n",
    "    if batch_size is None:\n",
    "        batch_size = train_params['model_params']['batch_size']\n",
    "    prod_rule_seq_list_train = prod_rule_seq_list[: train_params['num_train']]\n",
    "    prod_rule_seq_list_val = prod_rule_seq_list[train_params['num_train']\n",
    "                                                : train_params['num_train'] + train_params['num_val']]\n",
    "    prod_rule_seq_list_test = prod_rule_seq_list[train_params['num_train'] + train_params['num_val']\n",
    "                                                 : train_params['num_train']\n",
    "                                                 + train_params['num_val']\n",
    "                                                 + train_params['num_test']]\n",
    "    if target_val_list is None:\n",
    "        target_val_list_train = None\n",
    "        target_val_list_val = None\n",
    "        target_val_list_test = None\n",
    "    else:\n",
    "        target_val_list_train = target_val_list[: train_params['num_train']]\n",
    "        target_val_list_val = target_val_list[train_params['num_train']\n",
    "                                              : train_params['num_train'] + train_params['num_val']]\n",
    "        target_val_list_test = target_val_list[train_params['num_train'] + train_params['num_val']\n",
    "                                               : train_params['num_train']\n",
    "                                               + train_params['num_val']\n",
    "                                               + train_params['num_test']]\n",
    "    hrg_dataset_train = HRGDataset(hrg,\n",
    "                                   prod_rule_seq_list_train,\n",
    "                                   train_params['model_params']['max_len'],\n",
    "                                   target_val_list=target_val_list_train,\n",
    "                                   inversed_input=train_params['inversed_input'])\n",
    "    hrg_dataloader_train = DataLoader(dataset=hrg_dataset_train,\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=shuffle, drop_last=False)\n",
    "    if train_params['num_val'] != 0:\n",
    "        hrg_dataset_val = HRGDataset(hrg,\n",
    "                                     prod_rule_seq_list_val,\n",
    "                                     train_params['model_params']['max_len'],\n",
    "                                     target_val_list=target_val_list_val,\n",
    "                                     inversed_input=train_params['inversed_input'])\n",
    "        hrg_dataloader_label = DataLoader(dataset=hrg_dataset_val,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=shuffle, drop_last=False)\n",
    "    else:\n",
    "        hrg_dataset_val = None\n",
    "        hrg_dataloader_val = None\n",
    "    if train_params['num_test'] != 0 :\n",
    "        hrg_dataset_test = HRGDataset(hrg,\n",
    "                                      prod_rule_seq_list_test,\n",
    "                                      train_params['model_params']['max_len'],\n",
    "                                      target_val_list=target_val_list_test,\n",
    "                                      inversed_input=train_params['inversed_input'])\n",
    "        hrg_dataloader_test = DataLoader(dataset=hrg_dataset_test,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=shuffle, drop_last=False)\n",
    "    else:\n",
    "        hrg_dataset_test = None\n",
    "        hrg_dataloader_test = None\n",
    "    return hrg_dataloader_train, hrg_dataloader_label, hrg_dataloader_test, train_params\n",
    "\n",
    "\n",
    "def load_output():\n",
    "    input_dir_path = \"OUTPUT/data_prep_for_qm9\"\n",
    "    with gzip.open(os.path.join(input_dir_path, 'mhg_prod_rules.pklz'), \"rb\") as f:\n",
    "            hrg, prod_rule_seq_list = pickle.load(f)\n",
    "    return hrg, prod_rule_seq_list\n",
    "\n",
    "print(\"Start load input data and model\")\n",
    "\n",
    "hrg, prod_rule_seq_list = load_output()\n",
    "\n",
    "hrg_dataloader_train, hrg_dataloader_val, hrg_dataloader_test, Train_params \\\n",
    "    = get_dataloaders(hrg, prod_rule_seq_list, target_val_list = target_list)\n",
    "\n",
    "model_params = deepcopy(dict(Train_params['model_params']))\n",
    "\n",
    "mhg_vae = GrammarSeq2SeqVAE(\n",
    "    hrg=hrg,**model_params, use_gpu=True)\n",
    "\n",
    "mhg_vae = mhg_vae.to('cuda:0')\n",
    "\n",
    "print(\"load completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "vae_optimizer = optim.Adam(mhg_vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHG VAE Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from mhg.nn.loss import GrammarVAELoss_beta\n",
    "\n",
    "mhg_vae_loss_func = GrammarVAELoss_beta(hrg = hrg, beta = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta-TCVAE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "import torch\n",
    "\n",
    "def matrix_log_density_gaussian(x, mu, logvar):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates log density of a Gaussian for all combination of bacth pairs of\n",
    "    `x` and `mu`. I.e. return tensor of shape `(batch_size, batch_size, dim)`\n",
    "    instead of (batch_size, dim) in the usual log density.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "        Value at which to compute the density. Shape: (batch_size, dim).\n",
    "\n",
    "    mu: torch.Tensor\n",
    "        Mean. Shape: (batch_size, dim).\n",
    "\n",
    "    logvar: torch.Tensor\n",
    "        Log variance. Shape: (batch_size, dim).\n",
    "\n",
    "    batch_size: int\n",
    "        number of training images in the batch\n",
    "    \"\"\"\n",
    "    batch_size, dim = x.shape\n",
    "    x = x.view(batch_size, 1, dim)\n",
    "    mu = mu.view(1, batch_size, dim)\n",
    "    logvar = logvar.view(1, batch_size, dim)\n",
    "    return log_density_gaussian(x, mu, logvar)\n",
    "\n",
    "\n",
    "def log_density_gaussian(x, mu, logvar):\n",
    "    \"\"\"Calculates log density of a Gaussian.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor or np.ndarray or float\n",
    "        Value at which to compute the density.\n",
    "\n",
    "    mu: torch.Tensor or np.ndarray or float\n",
    "        Mean.\n",
    "\n",
    "    logvar: torch.Tensor or np.ndarray or float\n",
    "        Log variance.\n",
    "    \"\"\"\n",
    "    normalization = - 0.5 * (math.log(2 * math.pi) + logvar)\n",
    "    inv_var = torch.exp(-logvar)\n",
    "    log_density = normalization - 0.5 * ((x - mu)**2 * inv_var)\n",
    "    return log_density\n",
    "\n",
    "\n",
    "def log_importance_weight_matrix(batch_size, dataset_size):\n",
    "    \"\"\"\n",
    "    Calculates a log importance weight matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size: int\n",
    "        number of training images in the batch\n",
    "\n",
    "    dataset_size: int\n",
    "    number of training images in the dataset\n",
    "    \"\"\"\n",
    "    N = dataset_size\n",
    "    M = batch_size - 1\n",
    "    strat_weight = (N - M) / (N * M)\n",
    "    W = torch.Tensor(batch_size, batch_size).fill_(1 / M)\n",
    "    W.view(-1)[::M + 1] = 1 / N\n",
    "    W.view(-1)[1::M + 1] = strat_weight\n",
    "    W[M - 1, 0] = strat_weight\n",
    "    return W.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "def tc_decomposition_kl(n_data, latent_dist, latent_sample, alpha=1., beta=2., gamma=1., is_mss = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_data: int\n",
    "        Number of data in the training set\n",
    "\n",
    "    alpha : float\n",
    "        Weight of the mutual information term.\n",
    "\n",
    "    beta : float\n",
    "        Weight of the total correlation term.\n",
    "\n",
    "    gamma : float\n",
    "        Weight of the dimension-wise KL term.\n",
    "\n",
    "    is_mss : bool\n",
    "        Whether to use minibatch stratified sampling instead of minibatch\n",
    "        weighted sampling.\n",
    "\n",
    "    \"\"\"\n",
    "    batch_size, latent_dim = latent_sample.shape\n",
    "    log_pz, log_qz, log_prod_qzi, log_q_zCx = _get_log_pz_qz_prodzi_qzCx(n_data=n_data, latent_sample=latent_sample, latent_dist=latent_dist,\n",
    "                                                                                                  is_mss=is_mss)\n",
    "    \n",
    "    # I[z;x] = KL[q(z,x)||q(x)q(z)] = E_x[KL[q(z|x)||q(z)]]\n",
    "    mi_loss = (log_q_zCx - log_qz).mean()\n",
    "    # TC[z] = KL[q(z)||\\prod_i z_i]\n",
    "    tc_loss = (log_qz - log_prod_qzi).mean()\n",
    "    # dw_kl_loss is KL[q(z)||p(z)] instead of usual KL[q(z|x)||p(z))]\n",
    "    dw_kl_loss = (log_prod_qzi - log_pz).mean()\n",
    "    \n",
    "    # total loss\n",
    "    all_loss = alpha * mi_loss + beta * tc_loss + gamma * dw_kl_loss\n",
    "    \n",
    "    return all_loss, mi_loss, tc_loss, dw_kl_loss\n",
    "    \n",
    "# Batch TC specific\n",
    "# TO-DO: test if mss is better!\n",
    "def _get_log_pz_qz_prodzi_qzCx(n_data, latent_sample, latent_dist, is_mss=False):\n",
    "\n",
    "    batch_size, hidden_dim = latent_sample.shape\n",
    "\n",
    "    # calculate log q(z|x)\n",
    "    log_q_zCx = log_density_gaussian(latent_sample, *latent_dist).sum(dim=1)\n",
    "\n",
    "    # calculate log p(z)\n",
    "    # mean and log var is 0\n",
    "    zeros = torch.zeros_like(latent_sample)\n",
    "    log_pz = log_density_gaussian(latent_sample, zeros, zeros).sum(1)\n",
    "\n",
    "    mat_log_qz = matrix_log_density_gaussian(latent_sample, *latent_dist)\n",
    "\n",
    "    if is_mss:\n",
    "        # use stratification\n",
    "        log_iw_mat = log_importance_weight_matrix(batch_size, n_data).to(latent_sample.device)\n",
    "        mat_log_qz = mat_log_qz + log_iw_mat.view(batch_size, batch_size, 1)\n",
    "\n",
    "    log_qz = torch.logsumexp(mat_log_qz.sum(2), dim=1, keepdim=False)\n",
    "    log_prod_qzi = torch.logsumexp(mat_log_qz, dim=1, keepdim=False).sum(1)\n",
    "\n",
    "    return log_pz, log_qz, log_prod_qzi, log_q_zCx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## beta scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler_beta(epoch):\n",
    "    \n",
    "    if epoch == 1:\n",
    "        return 1.25\n",
    "    \n",
    "    elif 1.25 - (epoch - 1) * 0.1 > 0.75:\n",
    "        return 1.25 - (epoch - 1) * 0.1\n",
    "    \n",
    "    else:\n",
    "        return 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable, Function\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from mhg.nn.metric import LogRatio_loss\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def tgt_cor(latent_vec, tgt_vec):\n",
    "\n",
    "    tgt_pi = []\n",
    "    max_pi = 0\n",
    "    \n",
    "    latent_vec = latent_vec.cpu()\n",
    "    latent_vec = latent_vec.detach()\n",
    "    latent_vec = latent_vec.clone()\n",
    "    latent_vec = latent_vec.numpy()\n",
    "    \n",
    "    tgt_vec = tgt_vec.cpu()\n",
    "    tgt_vec = tgt_vec.detach()\n",
    "    tgt_vec = tgt_vec.clone()\n",
    "    tgt_vec = tgt_vec.numpy()\n",
    "    \n",
    "    latent_vec_t = latent_vec.T\n",
    "    tgt_vec = tgt_vec.reshape(64) #(batch_size,)\n",
    "    \n",
    "    for vec in latent_vec_t:\n",
    "        p_result = pearsonr(vec, tgt_vec)\n",
    "        tgt_pi.append(abs(p_result[0]))\n",
    "        \n",
    "    for pi in tgt_pi:\n",
    "        if max_pi < pi:\n",
    "            max_pi = pi\n",
    "    \n",
    "    max_pi = float(max_pi)\n",
    "    \n",
    "    return max_pi\n",
    "\n",
    "#train loop\n",
    "def train(epoch):\n",
    "    \n",
    "    mhg_vae.train()\n",
    "    \n",
    "    n_data = len(hrg_dataloader_train.dataset)\n",
    "\n",
    "    for each_idx, each_batch in enumerate(hrg_dataloader_train):\n",
    "    \n",
    "        print(\"epoch:\", epoch, \"batch:\",each_idx + 1,\"/\",len(hrg_dataloader_train))\n",
    "    \n",
    "        if len(each_batch[0]) < Train_params['model_params']['batch_size']:\n",
    "                            each_batch[0] = torch.cat([each_batch[0],\n",
    "                                                       Train_params['model_params']['padding_idx'] * torch.ones((Train_params['model_params']['batch_size'] - len(each_batch[0]),\n",
    "                                                                                  len(each_batch[0][0])), dtype=torch.int64)], dim=0)\n",
    "                            each_batch[1] = torch.cat([each_batch[1],\n",
    "                                                       Train_params['model_params']['padding_idx'] * torch.ones((Train_params['model_params']['batch_size'] - len(each_batch[1]),\n",
    "                                                                                  len(each_batch[1][0])), dtype=torch.int64)], dim=0)\n",
    "                            each_batch[2] = torch.cat([each_batch[2],\n",
    "                                                       torch.zeros((Train_params['model_params']['batch_size'] - len(each_batch[2])))], dim=0)\n",
    "                \n",
    "        if type(each_batch) == list:\n",
    "            \n",
    "            in_batch, out_batch, tgt_batch = each_batch\n",
    "            in_batch = torch.LongTensor(np.mod(in_batch, mhg_vae.vocab_size))\n",
    "            out_batch = torch.LongTensor(np.mod(out_batch, mhg_vae.vocab_size))\n",
    "            tgt_batch = torch.FloatTensor(tgt_batch)\n",
    "                  \n",
    "            tgt_batch = torch.reshape(tgt_batch, (-1, 1))\n",
    "           \n",
    "            in_batch = in_batch.to(\"cuda:0\")\n",
    "            out_batch = out_batch.to(\"cuda:0\")\n",
    "            tgt_batch = tgt_batch.to(\"cuda:0\")\n",
    "        \n",
    "            mhg_vae.init_hidden()\n",
    "            \n",
    "            #encode\n",
    "            mu, logvar = mhg_vae.encode(in_batch)\n",
    "            z = mhg_vae.reparameterize(mu, logvar)\n",
    "            \n",
    "            dist = mu, logvar\n",
    "        \n",
    "        ###### log ratio loss ######## \n",
    "\n",
    "        log_ratio_loss_score = LogRatio_loss(mu, tgt_batch)\n",
    "    \n",
    "        ###### mhg vae loss ######\n",
    "        \n",
    "        vae_optimizer.zero_grad()\n",
    "        \n",
    "        decoded = mhg_vae.decode(z, out_batch)\n",
    "        \n",
    "        pred_batch = decoded, mu, logvar\n",
    "        \n",
    "        beta_var = scheduler_beta(epoch)\n",
    "        \n",
    "        gamma = tgt_cor(mu, tgt_batch)\n",
    "\n",
    "        if 1 - gamma < 0.01:\n",
    "            gamma = 0.01\n",
    "            \n",
    "        elif 1 - gamma > 0.01:\n",
    "            gamma = 1 - gamma\n",
    "        \n",
    "        gamma_scale = 0.1\n",
    "    \n",
    "        default_beta = 0.01\n",
    "        mhg_vae_loss_score, reconst, kld = mhg_vae_loss_func(decoded, out_batch, mu, logvar, beta = default_beta)\n",
    "        \n",
    "        kl_decomposition_loss, mi_loss, tc_loss, dw_kl_loss = tc_decomposition_kl(n_data=n_data, alpha=0.75 , beta=beta_var, gamma=0.75, \\\n",
    "                                                                                latent_dist = dist, latent_sample = z, is_mss = True)\n",
    "        #### Loss function ####\n",
    "        mhg_vae_loss = reconst + kl_decomposition_loss + gamma_scale * gamma * log_ratio_loss_score\n",
    "    \n",
    "        mhg_vae_loss.backward()\n",
    "        vae_optimizer.step()\n",
    "        \n",
    "        mhg_vae_loss_score = mhg_vae_loss_score.cpu()\n",
    "        mhg_vae_loss = mhg_vae_loss.cpu()\n",
    "        reconst = reconst.cpu()\n",
    "        kl_decomposition_loss = kl_decomposition_loss.cpu() \n",
    "        mi_loss = mi_loss.cpu()\n",
    "        tc_loss = tc_loss.cpu()\n",
    "        dw_kl_loss = dw_kl_loss.cpu()\n",
    "        log_ratio_loss_score = log_ratio_loss_score.cpu()\n",
    "      \n",
    "\n",
    "        mhg_vae_loss_score = mhg_vae_loss_score.data.numpy()\n",
    "        mhg_vae_loss = mhg_vae_loss.data.numpy()\n",
    "        reconst = reconst.data.numpy()\n",
    "        kl_decomposition_loss = kl_decomposition_loss.data.numpy()\n",
    "        mi_loss = mi_loss.data.numpy()\n",
    "        tc_loss = tc_loss.data.numpy()\n",
    "        dw_kl_loss = dw_kl_loss.data.numpy()\n",
    "        log_ratio_loss_score = log_ratio_loss_score.data.numpy()\n",
    "        \n",
    "        save_path = \"OUTPUT/train/semiLogRatio_train_qm9_tcbeta\"\n",
    "        \n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "        \n",
    "        train_loss_log = \"epoch:\" + str(epoch) + \"/\" + str(num_epochs) + \" \" + \"vae train loss:\" + str(mhg_vae_loss_score) + \" \" + \"log-ratio loss:\"\\\n",
    "                            + str(log_ratio_loss_score) + \" \" + \"Reconst:\" + str(reconst) + \" \" + \"MI(x,z):\" + str(mi_loss) +\\\n",
    "                            \" \" + \"TC:\" + str(tc_loss) +\" \" + \"DW KLD:\" + str(dw_kl_loss) +\"\\n\"\n",
    "                            \n",
    "        if each_idx % 50 == 0:\n",
    "            with open(os.path.join(save_path, 'train_loss_log.txt'), mode='a') as f:\n",
    "                f.write(train_loss_log)\n",
    "                print(\"save train loss\")\n",
    "        \n",
    "        if each_idx % 100 == 0:\n",
    "            torch.save(mhg_vae, \"OUTPUT/train/semiLogRatio_train_qm9_tcbeta/mhg_vae_metric_qm9_save.pt\")\n",
    "\n",
    "def test(epoch):\n",
    "    \n",
    "    mhg_vae.eval()\n",
    "        \n",
    "    mhg_vae_test_loss = 0\n",
    "    reconst_test_loss = 0\n",
    "    kl_decomposition_test_loss = 0\n",
    "    mi_test_loss = 0\n",
    "    tc_test_loss = 0\n",
    "    dw_kl_test_loss = 0\n",
    "    log_ratio_test_loss = 0\n",
    "\n",
    "    n_data = len(hrg_dataloader_test.dataset)\n",
    "    \n",
    "    for each_idx, each_batch in enumerate(hrg_dataloader_test):\n",
    "            \n",
    "        print(\"epoch:\", epoch, \"batch:\", each_idx + 1, \"/\", len(hrg_dataloader_test))\n",
    "    \n",
    "        if len(each_batch[0]) < Train_params['model_params']['batch_size']:\n",
    "                            each_batch[0] = torch.cat([each_batch[0],\n",
    "                                                       Train_params['model_params']['padding_idx'] * torch.ones((Train_params['model_params']['batch_size'] - len(each_batch[0]),\n",
    "                                                                                  len(each_batch[0][0])), dtype=torch.int64)], dim=0)\n",
    "                            each_batch[1] = torch.cat([each_batch[1],\n",
    "                                                       Train_params['model_params']['padding_idx'] * torch.ones((Train_params['model_params']['batch_size'] - len(each_batch[1]),\n",
    "                                                                                  len(each_batch[1][0])), dtype=torch.int64)], dim=0)\n",
    "                            each_batch[2] = torch.cat([each_batch[2],\n",
    "                                                       torch.zeros((Train_params['model_params']['batch_size'] - len(each_batch[2])))], dim=0)\n",
    "                    \n",
    "        if type(each_batch) == list:\n",
    "            \n",
    "            in_batch, out_batch, tgt_batch = each_batch\n",
    "            in_batch = torch.LongTensor(np.mod(in_batch, mhg_vae.vocab_size))\n",
    "            out_batch = torch.LongTensor(np.mod(out_batch, mhg_vae.vocab_size))\n",
    "            tgt_batch = torch.FloatTensor(tgt_batch)\n",
    "        \n",
    "            tgt_batch = torch.reshape(tgt_batch, (-1, 1))\n",
    "          \n",
    "            in_batch = in_batch.to(\"cuda:0\")\n",
    "            out_batch = out_batch.to(\"cuda:0\")\n",
    "            tgt_batch = tgt_batch.to(\"cuda:0\")\n",
    "        \n",
    "            mhg_vae.init_hidden()\n",
    "        \n",
    "            mu, logvar = mhg_vae.encode(in_batch)\n",
    "            z = mhg_vae.reparameterize(mu, logvar)\n",
    "            \n",
    "            dist = mu, logvar\n",
    "\n",
    "        log_ratio_test_loss_score = LogRatio_loss(mu, tgt_batch)\n",
    "        \n",
    "        decoded = mhg_vae.decode(z, out_batch)\n",
    "        \n",
    "        pred_batch = decoded, mu, logvar\n",
    "        \n",
    "        default_beta = 0.01\n",
    "        beta_var = scheduler_beta(epoch)\n",
    "        \n",
    "        mhg_vae_loss, reconst, kld = mhg_vae_loss_func(decoded, out_batch, mu, logvar, beta = default_beta)\n",
    "\n",
    "        kl_decomposition_loss, mi_loss, tc_loss, dw_kl_loss = tc_decomposition_kl(n_data = n_data, alpha=0.75 , beta=beta_var, gamma=0.75,\\\n",
    "                                                                                   latent_dist = dist, latent_sample = z, is_mss = True)\n",
    "     \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        mhg_vae_loss = mhg_vae_loss.cpu()\n",
    "        reconst = reconst.cpu()\n",
    "        kl_decomposition_loss = kl_decomposition_loss.cpu() \n",
    "        mi_loss = mi_loss.cpu()\n",
    "        tc_loss = tc_loss.cpu()\n",
    "        dw_kl_loss = dw_kl_loss.cpu()\n",
    "        log_ratio_test_loss_score = log_ratio_test_loss_score.cpu()\n",
    "      \n",
    "\n",
    "        mhg_vae_loss = mhg_vae_loss.data.numpy()\n",
    "        reconst = reconst.data.numpy()\n",
    "        kl_decomposition_loss = kl_decomposition_loss.data.numpy()\n",
    "        mi_loss = mi_loss.data.numpy()\n",
    "        tc_loss = tc_loss.data.numpy()\n",
    "        dw_kl_loss = dw_kl_loss.data.numpy()\n",
    "        log_ratio_test_loss_score = log_ratio_test_loss_score.data.numpy()\n",
    "        \n",
    "        mhg_vae_test_loss += float(mhg_vae_loss)\n",
    "        reconst_test_loss += float(reconst)\n",
    "        kl_decomposition_test_loss += float(kl_decomposition_loss)\n",
    "        mi_test_loss += float(mi_loss)\n",
    "        tc_test_loss += float(tc_loss)\n",
    "        dw_kl_test_loss += float(dw_kl_loss)\n",
    "        log_ratio_test_loss += float(log_ratio_test_loss_score)\n",
    "\n",
    "    mhg_vae_test_loss /= len(hrg_dataloader_test.dataset)\n",
    "    reconst_test_loss /= len(hrg_dataloader_test.dataset)\n",
    "    kl_decomposition_test_loss /= len(hrg_dataloader_test.dataset)\n",
    "    mi_test_loss /= len(hrg_dataloader_test.dataset)\n",
    "    tc_test_loss /= len(hrg_dataloader_test.dataset)\n",
    "    dw_kl_test_loss /= len(hrg_dataloader_test.dataset)\n",
    "    log_ratio_test_loss /= len(hrg_dataloader_test.dataset)\n",
    "\n",
    "    save_path = \"OUTPUT/train/semiLogRatio_train_qm9_tcbeta\"\n",
    "    \n",
    "    test_loss_log = \"epoch:\" + str(epoch) + \"/\" + str(num_epochs) + \" \" + \"vae val losso:\" + str(mhg_vae_test_loss) + \" \" + \"log-ratio loss:\"\\\n",
    "                            + str(log_ratio_test_loss) + \" \" + \"Reconst:\" + str(reconst_test_loss) + \" \" + \"MI(x,z):\"\\\n",
    "                            + str(mi_test_loss) + \" \" + \"TC:\" + str(tc_test_loss) + \" \" + \"DW KLD:\" + str(dw_kl_test_loss) + \"\\n\"\n",
    "    \n",
    "    with open(os.path.join(save_path, 'test_loss_log.txt'), mode='a') as f:\n",
    "        f.write(test_loss_log)\n",
    "        print(\"save test loss\")\n",
    "    \n",
    "    return log_ratio_test_loss, mhg_vae_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 batch: 1 / 1562\n",
      "save train loss\n",
      "epoch: 1 batch: 2 / 1562\n",
      "epoch: 1 batch: 3 / 1562\n",
      "epoch: 1 batch: 4 / 1562\n",
      "epoch: 1 batch: 5 / 1562\n",
      "epoch: 1 batch: 6 / 1562\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-ffa92bd7f96b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmetric_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-fcc9911295e5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m###### log ratio loss ########\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mlog_ratio_loss_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogRatio_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m###### mhg vae loss ######\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/mhg_change/mhg/nn/metric.py\u001b[0m in \u001b[0;36mLogRatio_loss\u001b[0;34m(tensor, label)\u001b[0m\n\u001b[1;32m     58\u001b[0m                         \u001b[0mlog_ratio_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiff_log_tesor_dist\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdiff_log_label_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                         \u001b[0mlog_ratio_loss_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_ratio_loss_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_ratio_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "save_path = \"OUTPUT/train/semiLogRatio_train_qm9_tcbeta\"\n",
    "\n",
    "best_test_loss = 10000\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    train(epoch)\n",
    "    \n",
    "    metric_loss, vae_loss = test(epoch)\n",
    "    \n",
    "    test_loss = metric_loss + vae_loss\n",
    "    \n",
    "    save_model = \"OUTPUT/train/semiLogRatio_train_qm9_tcbeta/mhg_vae_metric_qm9_\" + str(epoch) + \"_model\" + \".pt\"\n",
    "    torch.save(mhg_vae, save_model)\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(mhg_vae, \"OUTPUT/train/semiLogRatio_train_qm9_tcbeta/mhg_vae_metric_qm9.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
